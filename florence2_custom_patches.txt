# Florence-2 Custom Patches

This project uses a locally patched version of the Florence-2 model. Due to several compatibility bugs in the official `microsoft/Florence-2-base` implementation regarding the `transformers` and `accelerate` libraries, the following modifications were made to the files in `src/resources/models/florence2/`.

## 1. Summary of Bug Fixes

| File | Line (approx) | Change | Bug Fixed |
| :--- | :--- | :--- | :--- |
| `configuration_florence2.py` | 265 | Added `self.forced_bos_token_id = None` | Fixed `AttributeError` for missing BOS token attribute. |
| `modeling_florence2.py` | 2533 | Added `_supports_sdpa = True` | Fixed crash where model class lacked the SDPA flag. |
| `modeling_florence2.py` | 557 | Added `device='cpu'` to `torch.linspace` | Fixed `RuntimeError: Cannot copy out of meta tensor`. |
| `config.json` | 1 | Added `"forced_bos_token_id": 2` | Ensured backward compatibility with expected BOS token. |
| `config.json` | `text_config` | Added `"model_type": "bart"` and architectures | Prevented library from falling back to `RobertaTokenizer`. |
| `tokenizer_config.json` | Global | Added `"image_token": "<unused0>"` | Fixed `AttributeError: RobertaTokenizer has no attribute image_token`. |
| `detector.py` | `detect_chart_roi` | Added `.to(self.model.dtype)` to `pixel_values` | Fixed `RuntimeError` due to Float32/Float16 mismatch. |
| `tokenizer.json` | Global | Expanded vocabulary to **51289**; injected BPE merges | Fixed `IndexError: Target index out of range` and `Missing vocab/merges`. |
| `config.json` | Global | Set `vocab_size` & `text_config.vocab_size` to **51289** | Aligned architecture with the expanded 51289 weight matrix. |
| `processing_florence2.py` | 90 | Wrapped `tokenizer.additional_special_tokens` in `getattr()` | Fixed `AttributeError: RobertaTokenizer has no attribute additional_special_tokens`. |
| `detector.py` | `detect_chart_roi` | Added `use_cache=False` to `model.generate()` | Fixed `TypeError: 'EncoderDecoderCache' object is not subscriptable`. |
| detector.py | _load_model | Set hf_logging to ERROR | Fixed console hang/deadlock caused by deprecated AttentionMaskConverter warning. |

## 2. Implementation & Deployment Strategy

### Local Execution
To ensure these fixes are respected during development, the `AutoModel` loader in `detector.py` is configured with:
- `trust_remote_code=True`: Required for custom Florence-2 logic.
- `local_files_only=True`: Prevents overwriting patches with broken remote versions.
- **Weight Management**: Removed `pytorch_model.bin` to favor `model.safetensors`, preventing load report conflicts.

### Packaging for Distribution
For the final installer/executable:
1. **Bundling**: The patched `florence2` directory is included in the application's internal `resources` folder.
2. **Offline Mode**: The application is forced into "Offline Mode" via `HF_HUB_OFFLINE=1` to ensure it never attempts to repair the cache.
3. **Relative Paths**: The model path is resolved dynamically relative to the executable's location.

### Vocabulary & Tokenization Integrity
- **Atomicity**: Coordinates `<0>` through `<1023>` are registered as **Special Added Tokens** in `tokenizer.json`. This forces the tokenizer to treat them as single IDs (50265â€“51288) rather than strings of characters, which is required for spatial grounding.
- **Manual Instantiation**: Due to strict attribute checking in `AutoProcessor`, the `detector.py` manually instantiates `Florence2Processor` using a `BartTokenizer` to ensure the extended vocabulary is handled correctly without library-level validation crashes.

---
*Note: These patches should be re-evaluated if a major update to the Florence-2 repository is released by Microsoft.*